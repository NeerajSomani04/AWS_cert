Overall S3 Structure --> (simple storage service)
  1. S3 Essential point
  2. S3 Components --> Buckets, Folders, Objects
  3. S3 Features --> Objects Versioning, Storage Classes, Storage Lifecycle policies, S3 Events, Static Web Hosting, Encryption
  4. S3 Permissions and S3 performance
Below will be detailed summary of above listed 4 points:

1. S3 Essentials --> This is AWS main storage service and server many purposes:
  Bulk (unlimited) static object storage
  various storage classes to optimize cost vs needed object availability and durability
  object versioning
  Access restrictions via object bucket policies/permissions
  Object management via lifecycle policies
  Hosting static files and websites
  Origin for cloudFront CDN (content delivery network)
  File sharing or back-up/archiving for hybrid networks (via AWS Storage Gateway)
  A Bucket can contain both encrypted or non-encrypted objects. Encryption is available at object level. while versioning is available at bucket level.

Few important facts -->
  1. S3 objects stays within region but synced across AZs for high availability (99.99%) and durability (11x9s)
  2. always create S3 bucket in a region where it can best server its purpose:
    --> serving fast content to customer
    --> sharing data with EC2
    
S3 Read consistency rule --> in all regions
  read-after-write --> PUT request for new object
  Eventual consistency --> PUT for overwriting existing objects and DELETE for objects

S3 Errors --> generally handled with http response
  404 --> Not found
  403 --> Forbidden (no access)
  400 --> Bad request
  500 --> server error

S3 Event Notification --> using AWS services like SNS, Lambda, SQS

2. S3 Components ---(S3 bucket, S3 Object, S3 folders) -------------------------
S3 Buckets --> (default 100 bucket per account)
  Buckets are main storage container of S3. grouping of object can be done by using "folder" concept.
  tags can be used to organize buckets
  encrypted by default
  S3 buckets and objects resides within region where it got created, untill explicitly transfered.
    This brings low latency, minimize cost and address regulatory requirements
 
 Bucket Naming convention -->
    Every bucket must have unique name across all of AWS.
    must comply with DNS naming convention
    charachter limit (min 3 - max 63)
    must start with letter or number and can only contain lowercase letters, numbers, period, hyphen
    No uppercase letters or numbers
    Also, two symbols cant be together, means, hyphen can't follow period and vice versa
    must not be an IP address format
 Buckets limitation -->
    only 100 buckets per AWS account, Bucket ownership can't be transferd, Bucket can hold unlimited objects

S3 Objects -->
  Objects are static files and metadata information.
  Metadata means - storage type, encryption settings and permissions. (in the form of key-value pair)
  Storage type (class) is must and used to determines objects availability, durability, and cost.
  By default, all objects are private.
  
  Objects --> 0 byte to 5 TB size
    multiple versions (if versioning enabled)
    every object can be access via URL
    Storage Lifecycle policy can be applied (eg:- stored in Standard, then moved to IA, then moved to Glacier)
    Encryption can be done
    Objects Organized into "Sub-name" spaces known as folders
  
  Objects Encryption -->
    SSE (Server Side Encryption) --> AWS provide - AES-256, encryption at S3 level
    Client Side Encryption --> Client uses 3rd party tool, or some other encryption technology

S3 Folders --> ( this is just a concept AWS support, but in general S3 has a flat structure, means no hirarchy, like any other file system.
    --> it can be used as a means of grouping objects together
    --> it can be created by using key-name prefixes for objects

Before Understanding each parts of S3, lets understand how data gets to S3 or to AWS storage layer
------- using Storage "Transit" services ---- "Moving Data to S3"
1. Multi-part Upload Benefits --> 
    --> allows to upload a single object as a set of parts
    --> allows to upload part concurrently (Asynchronously) -- this can speed up the overall process
    --> allows to start/stop/resume file upload
    --> if tranmission of any part fails, that part alone can be retransmitted
    --> S3 reassembles each part after complete upload and creates one object 
    --> Multipart approach is must for file size 5GB and larger but suggested to implement this for object size 100 MB or larger
    --> file size limitation of S3 is 0 byte to 5 TB
    
    S3 Transfer Acceleration --> 
        --> Users can upload content through cloudFront Edge location
        --> and can change end-point URL
        example:- mybucket.s3.amazonaws.com  --------->   mybucket.s3-accelerate.amazonaws.com
        
2. Signle Operation upload -->
    --> Single opration upload is traditional approach for uploading file as One part
    --> Single operation can support a file size upto 5 GB, but suggest to implement mutipart upload for any file size 100 MB or larger 

3. Storage Gateway --> connect local data center software (VMWare or Hyper-V) appliances to cloud based storage such as AWS S3
     a. Volume Gateway -->
       Gateway-Cached Volume --> 
        --> create storage volumes and mount them as iSCSI devices on on-premises server
        --> Gateway will store the data to AWS S3 that is written to this volume and cache frequently accessed data on-premises in the storage server.
      Gateway-Stored Volume --> store all the data locally (on-premises) in storage volume and incrementally take snapshots of data for back-up and store them to S3.
    b. File Gateway --> local NFS, object are stored and retreivable from S3
    c. Tape Gateway --> industry standard iSCSI virtual tape library devices 
                    --> common back-up appliances (eg. Veeam, veritas, Dell, etc)
                    
4. Snowball --> An AWS service/device for petabyte scale of data transportation from on-premises to cloud and vice versa.
        --> AWS provided secure transfer appliances
        --> use up to 80 TB per device
    --> 4.a --> Snowball Edge --> snowball plus onboard compute capability
                              --> upto 100 TB per device
                              --> can be clustered
                              --> S3 API interface
                              --> Lambda functions
    --> 4.b. --> SnowMobile --> ExaByte-scale data transfer (100 PB per Snowmobile)
    
5. AWS S3 Import/Export --> gives the ability to import/export on-premises data and physically mail to AWS (using a device owned by customer)
      --> AWS will import the data to S3, Glacier, EBS, within one business day
      --> benfits --> off-site backup policy, Disaster Recovery option, easy and quick migration of LARGE (16 TB per job) amount of data to/from cloud

--------------- S3 Features and oprational facts -------
1. S3 Performance --> S3 can scale upto very high request rates, best practice is to avoid account limits and optimize solution
    --> contact AWS support for temporary limit concerns, below is what default limits:
          300 request per sec --> PUT/LIST/DELETE
          800 request per sec --> GET
    --> revisit design or follow best practices incase you consistently hit high number of request to optimize solution 
         100 request per sec --> PUT/LIST/DELETE
         300 request per sec --> GET
  --- 2 types of workload options ---
    1. S3 Intensive Mixed request workload (PUT/LIST/DELETE/GET) -->
  solution --> Introducing randomness to avoid overwhelming S3 partitions 
      --> S3 automatically creates and maintain an index for S3 key-names in each AWS region
      --> The keys are stored across multiple partitions based on key-names
      --> adding random prefix can help spread load across
      --> prefix can be generated by using MD5 hash-code or any other approaches
      --> these preix can be added to the object name itself or to the key-name folder itself. it cant be added to bucket name.
    2. S3 GET Intensive Workload -->
      --> Same above, Introducing Randomness can help
      --> Use of AWS CloudFront -->
          --> As its Content Delivery Network, which delivers content with low latency and high transfer rate.
          --> Cache S3 objects, CloudFront will be closer to user location
    
2. S3 Permissions --> By default all buckets and objects are private only owner has access
  owner can give permissions using resource based policies (bucket level or S3 ACLs) or IAM policies
    1. IAM policies --> attached to users, groups, or/and roles
            --> can't attach to S3 bucket or objects
            --> can't grant access to anonymous user
            --> {JSON} and can contain details about S3 bucket/object using ARN (amazon resource name)
                  {arn:aws:s3:::bucket_name/object_name} // object name
                  {arn:aws:s3:::bucket_name} // bucket name
                  {arn:aws:s3:::bucket_name/*} // all objects inside bucket
                  {arn:aws:s3:::*} // all buckets
                  {arn:aws:s3:::bucket_name/object_name/${aws:user_name}} // using variable inside json
                  
  Resource-based policies - 2 ways -->
    
    2. Bucket Policies --> {JSON} attached to S3 bucket level (not to objects), means its automatically applied to all objects of bucket created by bucket owner. objects created by other than owner has to explicitly specify object level permission.
    --> can be used to grant accesss to IAM users or other AWS accounts
    --> policy specifies allow and deny permission, even can specify based on user as well. (like, which user can use PUT/DELETE, etc)
    
    3. S3 ACL Policies --> {XML} can be used to specify permissions at both bucket and object level
          --> ACLs can't deny permissions and can't grant conditional permissions
          --> most useful for Object level permissions, as this is the only way to attach a policy to objects

---- S3 Features --->
1. S3 Encryption --> 2 ways -- All handled in background by AWS SDK
A Bucket can contain both encrypted or non-encrypted objects. Encryption is available at object level. while versioning is available at bucket level.

   1. In-transit/client side --> SSL or client side encyption techniques
        1. Client Side Master Key (CSMK) --> client manage master key and unencrypted data never sent to AWS.
              upload/download steps with CSMK --> 
                  Random "Data key" generated at S3 client
                  Earlier generated Random "Data key" will be encrypted with client provided "CSMK"
                  S3 client encrypt the data using "data key"
                  data will be uploaded with details as object metadata (x-amz-meta-x-amz-key)
                  client download data file with its metadata
                  metadata will inform S3 client to use which "CSMK" to decrypt "data key"
                  "data key" is used to decrypt actual data objects
        2. AWS KMS-managed customer master key (CMK) --> 
              --> client gets a unique encryption key for each object({(1)application request}, {(2) response: key and cipherText Blob})
              --> a plaintext key used to encrypt objet data
              --> encypted data will be uploaded with a cipherText blob as object Metadata
              --> Download file and metadata
              --> KMS decrypts ciphertext blob for client, to get the plaintext key
              --> plaintext key is used to decrypt the object data file
  
  2. At rest / server side --> S3 encrypt data for user / server side encryption
      SSE-S3 --> Amazon S3 provides encryption key before saving data to disk 
            --> AES-256 (Advanced Encryption Standards)
            --> each object gets a unique key that is encrypted by regulatory-rotated master key
            --> every upload request will have request header as "x-amz-server-side-enryption"
      SSE-KMS --> similar as SSE-S3, but key managed by AWS-KMS service, provides better control, better auditing for access
      SSE-C --> customer provides and manages master key, S3 encrypts and decrypts using that key

2. S3 versioning --> a feature to keep old/new/deleted S3 objects
  by default, its disabled. But once enabled, you can only suspend it, can't disable it.
  Hence, suspending versioning mean, not creating any new version, but old versions will be maintained as-is
  versioning can only be set at bucket level and will be applicable to all objects
  Lifecycle policies can be applied to specific version of the objects
  every object is independent in itself and consist of unique version id
  different life-cycle policy can be applied to current version and previous versions
  versioning & life-cycle policy can both be enabled at the same time. Used this feature to create great archival and back-up solution.
  Cross Region Replication is also available.
  Enable Versioning -->
     Existing object remain unchaged and have null as versionID
     Each addedd objects will automatically have unique versionID
     VersionID are generally generated by S3
     Every new version is billed as full new S3 object
     Each version object is independent from other object and complete in itself not a partial part of object
     Any object can be fetched using object key and VersionID
  Deleting Versioned objects --> 
    1. if delete is made to an object without specifying version id, then AWS will add a delete marker to all versions of object
    2. when we try to access object, S3 will response 404, but you can retrieve specific version of object by specifying version id
    3. each version of object can permanently be deleted only by specifying version id
  Restoring versioned objects --> means making old version of object to current, can be achieve in 2 ways
    Deleting current versions --> keep deleting untill you get needed object version
    copying old version --> each copied objects (even from same bucket) will become new version of object with unique version id

3. S3 storage classes and Glacier -->
  storage class --> represents classification of object in S3, this dictates storage cost, availability, durability, frequency of access
              --> each object must be assigned to a storage class.
      Durability --> can be described a the probability that you will eventually be able to get your object back from the storage system from one of the stores and archives.
      Availability --> is the probability that you will be able to get it back the moment that you ask for it.
  4 class types -->
  S3-Standard --> default and for all purpose, 11X9's durable, 99.99% available, most expensive 
  S3-RRS (Reduced Redundency Storage)--> designed for non-critical/reproducible objects. 99.99% available/durable, little less expensive
  S3-IA (Infrequent Access) {standard-IA} --> designed for IA but with immidiate availability when requested, 11X9's durable, 99.90% available, less expensive than standard/RRS. (30 days minimum storage)
    OneZone-IA --> all S3-IA specification applied to one specific zone only. 11X9's durable, 99.5% available, (30 days minimum storage)
    S3-Glacier --> long time archival storage, should not be used for back-ups, cheapest, several hours of retrieval time, 11X9's durable. (90 days minimum storage), SSL/TLS end point, encryption at rest, 
          --> 40 TB max object size, Vault lock feature (just like safe house lock).
   3 types of retrieval --> expedited (1-5 min), standard (3-5 hr), bulk (5-12 hr)

Settings/changing storage class --> while uploading to S3, default storage class is Standard.
    you can perform manual setting or use life cycle policy to update/change storage class for object.
    ** to move to Glacier storage class, can be done only by lifecycle policy and it can take 1-2 days to take effect.

4. S3 lifecycle policy --> (by default disabled), set of rules that automates migration of objects to various storage classes
      --> good for company data retention policy, cost efficient, ease of management, 
      --> each object can have their own life-cycle policy, even different version of same object can have different policy

5. S3 hosting static website --> you can build a code that can navigate to different files (usually HTML, CSS, JavaScript) stored at S3.
      --> {required} index.html (or default file) and {option} error.html (error file for 4XX errors) .
       URL format --> bucket_name.s3-website(- or.)region.amazonaws.com
       URL example --> my-cool-site.s3-website-us-east-1.amazonaws.com
    --> AWS Route 53 can be used to map human redable domain names with this URL, also helpful in DNS failover situation
          Required configurations:
              Enabling Website Hosting
              Configuring Index Document Support
              Permissions Required for Website Access
          Optional configurations:
              (Optional) Configuring Web Traffic Logging
              (Optional) Custom Error Document Support
              (Optional) Configuring a Webpage Redirect
    
  Cross Origin Resource Sharing (CORS) --> settings at S3 bucket level, {XML} file
    a method that allow web application located at one domain to access and use resources at another domain
    or in AWS world, one S3 bucket can access resources of another S3 bucket
    Just need to Enabling CORS at S3 bucket level 
    
6. S3 Events --> used to setup automated notifications between S3 and other AWS services/resources based on some event occurance.
    common example:- RRSObjectLost (used for automating the recreation of lost RRS objects)
                     ObjectsCreated (in-case of any of these API calls -- PUT/POST/COPY/CompleteMultiPartUpload
    --> Event notification can be send to following AWS services (SNS/Lambda/SQS Queue)

when can we use Amazon S3 Transfer Acceleration?
    --> You have customers that upload to a centralized bucket from all over the world.
    --> You transfer gigabytes to terabytes of data on a regular basis across continents.
    --> You are unable to utilize all of your available bandwidth over the Internet when uploading to Amazon S3.
following steps needs to be done to set-up this -->
  --> Enable Transfer Acceleration on a bucket – 
  --> make sure bucket name must not conatin period (".")
  example for S3-accelerate endpoint --->
      bucketname.s3-accelerate.amazonaws.com – to access an acceleration-enabled bucket.
  OR    bucketname.s3-accelerate.dualstack.amazonaws.com
  
--- from Route53 section -----------
S3 for DNS failover -->
  --> By using a failover routing policy in a Route53 DNS record set, an S3 bucket can be used as a failover endpoint.
  --> this can provide an extremely reliable back-ups solution if your primary end-point fails.
  --> even though S3 can only be used for static web hosting, it gives you the opportunity to provide your users with sometype of information untill the primary endpoint is working again.
  --> An S3 bucket can also be used as a primary endpoint, if you just want to host a simple static site.

**** Note --> for a DNS record to use an S3 bucket as an endpoint, the bucket name must be the same as the Domain name.

Troubleshooting Amazon S3 --> 
    Symptoms --> 
       1. --> Significant Increases in HTTP 503 Responses to Requests to Buckets with Versioning Enabled
            --> this usually comes for PUT or DELETE requests
            --> this occurs When you have objects with millions of versions
S3 Inventory tool --> To determine which S3 objects have millions of versions, use the Amazon S3 inventory tool. The inventory tool generates a report that provides a flat file list of the objects in a bucket.
            --> S3 team encourages customers to investigate applications that repeatedly overwrite the same S3 object

       2. --> Unexpected Behavior When Accessing Buckets Set with CORS
            --> Verify that the CORS configuration is set on the bucket.
            --> Capture the complete request and response using a tool of your choice. 
                --> Verify that the request has the Origin header.
                --> Verify that the Origin header in your request matches at least one of the AllowedOrigin elements in the specified CORSRule.
                --> Verify that the method in your request (or in a preflight request, the method specified in the Access-Control-Request-Method) is one of the AllowedMethod elements in the same CORSRule.
                --> For a preflight request, if the request includes an Access-Control-Request-Headers header, verify that the CORSRule includes the AllowedHeader entries for each value in the Access-Control-Request-Headers header.
