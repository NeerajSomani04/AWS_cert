Overall S3 Structure -->
  1. S3 Essential point
  2. S3 Components --> Buckets, Folders, Objects
  3. S3 Features --> Objects Versioning, Storage Classes, Storage Lifecycle policies, S3 Events, Static Web Hosting, Encryption
  4. S3 Permissions and S3 performance
Below will be detailed summary of above listed 4 points:

1. S3 Essentials --> This is AWS main storage service and server many purposes:
  Bulk static object storage
  various storage classes to optimize cost vs needed object availability and durability
  object versioning
  Access restrictions via object bucket policies/permissions
  Object management via lifecycle policies
  Origin for cloudFront CDN (content delivery network)
  File sharing or back-up/archiving for hybrid networks (via AWS Storage Gateway)

Few important facts -->
  1. S3 objects stays within region but synced across AZs for high availability, durability
  2. always create S3 bucket in a region where it can best server its purpose:
    --> serving fast content to customer
    --> sharing data with EC2
    
S3 Read consistency rule --> in all regions
  read-after-write --> PUT request for new object
  Eventual consistency --> PUT for overwriting existing objects and DELETE for objects

S3 Errors --> generally handled with http response
  404 --> Not found
  403 --> Forbidden
  400 --> Bad request
  500 --> server error
S3 Event Notification --> using AWS services like SNS, Lambda, SQS

2. S3 Components ----------------------------
S3 Buckets -->
  Buckets are main storage container of S3.
  S3 buckets and objects resides within region where it got created, untill explicitly transfered.
    This brings low latency, minimize cost and address regulatory requirements
 Bucket Naming convention -->
    Every bucket must have unique name across all of AWS.
    must comply with DNS naming convention
    charachter limit ( min 3 - max 63)
    must start with letter or number and can only contain lowercase letters, numbers, period, hyphen
    Also, two symbols cant be together, means, hyphen can't follow period and vice versa
    must not be an IP address format
 Buckets limitation -->
    100 per AWS account, Bucket ownership cant be transferd, Bucket can hold unlimited objects

S3 Objects -->
  Objects are static files and metadata information.
  Metadata means - storage type, encryption settings and permissions.
  Storage type is must and used to determines objects availability, durability, and cost.
  By default, all objects are private.
  Objects --> 0 byte to 5 TB size
    multiple versions (if versioning enabled)
    every object can be access via URL
    Storage Lifecycle policy can be applied (eg:- stored in Standard, then moved to IA, then moved to Glacier)
    Encryption can be done
    Objects Organized into "Sub-name" spaces known as folders
  Objects Encryption -->
    SSE (Server Side Encryption) --> AWS provide - AES-256, encryption at S3 level
    Client Side Encryption --> Client uses 3rd party tool, or some other encryption technology
S3 Folders --> ( this is just a concept AWS support, but in general S3 has a flat structure, means no hirarchy, like any other file system.
    --> it can be used as a means of grouping objects together
    --> it can be created by using key-name prefixes for objects

Before Understanding other parts of S3, lets understand how data gets to S3 or to AWS storage layer
------- using Storage "Transit" services ---- "Moving Data to S3"

1. Multi-part Upload Benefits --> 
    --> allows to upload a single object as a set of parts
    --> allows to upload part concurrently (Asynchronously) -- this can speed up the overall process
    --> allows to start/stop/resume file upload
    --> if tranmission of any part fails, that part alone can be retransmitted
    --> S3 reassembles each part after complete upload and creates one object 
    --> Multipart approach is must for file size 5GB and larger but suggested to implement this for object size 100 MB or larger
    --> file size limitation of S3 is 0 byte to 5 TB
2. Signle Operation upload -->
    --> Single opration upload is traditional approach for uploading file as One part
    --> Single operation can support a file size upto 5 GB, but suggest to implement mutipart upload for any file size 100 MB or larger 
3. Storage Gateway --> used to control traffic for storage services like S3
    Gateway-Cached Volume --> frequently accessed data will be cached to speed up the proces
    Gateway-Stored Volume --> store all the data to this volume and incrementally take snapshots for back-up and store to S3.
 Snowball --> An AWS service for petabyte scale of data transportation from on-premises to cloud.
AWS S3 Import/Export --> this will also gives the ability to move and extract data to/from cloud

S3 Performance --> contact AWS support for temporary limit concerns, below is what default temp limit:
  300 request per sec --> PUT/LIST/DELETE
  800 request per sec --> GET
  
  Need to revisit design or follow best practices (to improve performance) if any of below happening with high number of consistency:
     100 request per sec --> PUT/LIST/DELETE
     300 request per sec --> GET
     
  S3 - Introducing Randomness to avoid overwhelming S3 partitions -->
    S3 automatically creates and maintain an index of S3 key-names in AWS region, hence prefix object names with some randomness 
    will avoid imbalance partition.
  S3 GET intesive workload --> best practice approach
    use randomness
    use Amazon CloudFront, because CloudFront is Content Delivery Network, which delivers content with low latency and high transfer rate.
    
S3 Permissions --> By default all buckets and objects are private.
  owner can give permissions using resource policies (bucket level or S3 ACLs) or IAM policies
  IAM policies --> can contain details about S3 object ARN {arn:aws:s3:::bucket_name/object_name} 
  Bucket Policies --> {JSON} attached to S3 buckets level, means its automatically applied to all objects of bucket created by bucket owner. objects created by other user has to explicitly specify object level permission.
    these can be used to assign to IAM users or other AWS account holders.
  S3 ACL Policies --> {XML} permissions at bucket and object level
    ACLs can't deny permissions and can't grant conditional permissions
    
S3 Encryption --> All handled by AWS SDK in background
  In-transit --> SSL or client side
    Client Side Master Key (CSMK) --> 
      client manage master key and never shares with AWS.
      upload/download steps with CSMK --> 
          Random Data key generated at S3 client
          data key is encrypted with client CSMK
          encrypt the data with data key
          upload file and metadata
          download file and metadata
          decrypt data key with CSMK
          data key used to decrypt objects
    AWS KMS-managed customer master key (CMK) --> 
      client gets a unique encryption key for each object ({(1)application request}, {(2) response: key and cipherText Blob})
          --> a plaintext key used to encrypt objet data
          --> a cipherText blob to upload to S3 as object Metadata
      (3) Encrypt the file --> plainText key used to encrypt file
      (4) upload file and metadata --> client uploads the encrypted file to S3 with cipherText blob as object MetaData.
      (5) Download file and metadata
      (6) KMS decrypts ciphertext for client, to get the plaintext
      (7) plaintext key is used to decrypt the object
  At rest --> S3 encrypt data for user
    SSE-S3 --> AES-256 (Advanced Encryption Standards), encryption before saving to disk
      each object gets a unique key that is encrypted by regulatory-rotated master key
    SSE-KMS --> similar as SSE-S3, but key managed by KMS
    SSE-C --> customer provides and manages master key, S3 encrypts and decrypts using that key

S3 versioning --> a feature to keep old/new/deleted S3 objects
  by default, its disabled. But once enabled, you can only suspend it, can't disable it.
  versioning can only be set at bucket level and will be applicable to all objects
  Lifecycle policy can be applied to specific version of the objects
  every object is independent in itself and consist of unique version id
  
  Delete Versioned objects --> 
    1. if delete is made to an object without specifying version id, then AWS will add a delete marker to all versions of object
    2. when we try to access object, S3 will response 404, but you can retrieve specific version of object by specifying version id
    3. each version of object can permanently be deleted by specifying version id
  Restoring versioned objects --> means making old version of object to current, can be achieve in 2 ways
    Deleting current versions --> keep deleting untill you get needed object version
    copying old version --> each copied objects (even from same bucket) will become new version of object with unique version id

S3 storage classes and Glacier -->
  storage class --> represents classification of object in S3, this dictates storage cost, availability, durability, frequent of access
  S3-Standard --> default and for all purpose, 11X9's durable, 99.99% available, most expensive 
  S3-RRS (Reduced Redundency Storage)-->designed for non-critical/reproducible objects. 99.99% available/durable, little less expensive
  S3-IA (Infrequent Access) {standard-IA} --> designed for IA but with immidiate availability when requested, 11X9's durable, 99.90% available, less expensive than standard/RRS
    OneZone-IA --> all specification applied to one specific zone only
  S3-Glacier --> long time archival storage, not for back-ups, cheapest, several hours of retrieval time, 11X9's durable
        3 types of retrieval --> expedited (1-5 min), standard (3-5 hr), bulk (5-12 hr)

S3 lifecycle policy --> (by default disabled), set of rules that automates migration of objects to various storage classes

S3 hosting static website --> you can build a code that can nevigate to different files stored at S3.
  index.html (or default file) and error.html (error file) must be specified.
  URL example --> bucket_name.s3-website(- or.)region.amazonaws.com
  
  Cross Origin Resource Sharing (CORS) --> settings at S3 bucket level, {XML} file
    a method that allow web application located at one domain to access and use resources at another domain
    or in AWS world, one S3 bucket can access resources of another S3 bucket
    Enabling CORS --> 
    


