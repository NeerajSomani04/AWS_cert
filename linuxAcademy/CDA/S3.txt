Overall S3 Structure -->
  1. S3 Essential point
  2. S3 Components --> Buckets, Folders, Objects
  3. S3 Features --> Objects Versioning, Storage Classes, Storage Lifecycle policies, S3 Events, Static Web Hosting, Encryption
  4. S3 Permissions and S3 performance
Below will be detailed summary of above listed 4 points:

1. S3 Essentials --> This is AWS main storage service and server many purposes:
  Bulk static object storage
  various storage classes to optimize cost vs needed object availability and durability
  object versioning
  Access restrictions via object bucket policies/permissions
  Object management via lifecycle policies
  Origin for cloudFront CDN (content delivery network)
  File sharing or back-up/archiving for hybrid networks (via AWS Storage Gateway)

Few important facts -->
  1. S3 objects stays within region but synced across AZs for high availability, durability
  2. always create S3 bucket in a region where it can best server its purpose:
    --> serving fast content to customer
    --> sharing data with EC2
    
S3 Read consistency rule --> in all regions
  read-after-write --> PUT request for new object
  Eventual consistency --> PUT for overwriting existing objects and DELETE for objects

S3 Errors --> generally handled with http response
  404 --> Not found
  403 --> Forbidden
  400 --> Bad request
  500 --> server error
S3 Event Notification --> using AWS services like SNS, Lambda, SQS

2. S3 Components ----------------------------
S3 Buckets -->
  Buckets are main storage container of S3.
  S3 buckets and objects resides within region where it got created, untill explicitly transfered.
    This brings low latency, minimize cost and address regulatory requirements
 Bucket Naming convention -->
    Every bucket must have unique name across all of AWS.
    must comply with DNS naming convention
    charachter limit ( min 3 - max 63)
    must start with letter or number and can only contain lowercase letters, numbers, period, hyphen
    Also, two symbols cant be together, means, hyphen can't follow period and vice versa
    must not be an IP address format
 Buckets limitation -->
    100 per AWS account, Bucket ownership cant be transferd, Bucket can hold unlimited objects

S3 Objects -->
  Objects are static files and metadata information.
  Metadata means - storage type, encryption settings and permissions.
  Storage type is must and used to determines objects availability, durability, and cost.
  By default, all objects are private.
  Objects --> 0 byte to 5 TB size
    multiple versions (if versioning enabled)
    every object can be access via URL
    Storage Lifecycle policy can be applied (eg:- stored in Standard, then moved to IA, then moved to Glacier)
    Encryption can be done
    Objects Organized into "Sub-name" spaces known as folders
  Objects Encryption -->
    SSE (Server Side Encryption) --> AWS provide - AES-256, encryption at S3 level
    Client Side Encryption --> Client uses 3rd party tool, or some other encryption technology
S3 Folders --> ( this is just a concept AWS support, but in general S3 has a flat structure, means no hirarchy, like any other file system.
    --> it can be used as a means of grouping objects together
    --> it can be created by using key-name prefixes for objects

Before Understanding each parts of S3, lets understand how data gets to S3 or to AWS storage layer
------- using Storage "Transit" services ---- "Moving Data to S3"
1. Multi-part Upload Benefits --> 
    --> allows to upload a single object as a set of parts
    --> allows to upload part concurrently (Asynchronously) -- this can speed up the overall process
    --> allows to start/stop/resume file upload
    --> if tranmission of any part fails, that part alone can be retransmitted
    --> S3 reassembles each part after complete upload and creates one object 
    --> Multipart approach is must for file size 5GB and larger but suggested to implement this for object size 100 MB or larger
    --> file size limitation of S3 is 0 byte to 5 TB
2. Signle Operation upload -->
    --> Single opration upload is traditional approach for uploading file as One part
    --> Single operation can support a file size upto 5 GB, but suggest to implement mutipart upload for any file size 100 MB or larger 
3. Storage Gateway --> connect local data center software appliances to cloud based storage such as AWS S3
    Gateway-Cached Volume --> 
        --> create storage volumes and mount them as iSCSI devices on on-premises server
        --> Gateway will store the data to AWS S3 that is written to this volume and cache frequently accessed data on-premises in the storage server.
    Gateway-Stored Volume --> store all the data locally (on-premises) in storage volume and incrementally take snapshots of data for back-up and store them to S3.
4. Snowball --> An AWS service/device for petabyte scale of data transportation from on-premises to cloud and vice versa.
5. AWS S3 Import/Export --> gives the ability to import/export on-premises data and physically mail to AWS (using a device owned by customer)
      --> AWS will import the data to S3, Glacier, EBS, within one business day
      --> benfits --> off-site backup policy, Disaster Recovery option, easy and quick migration of LARGE (16 TB per job) amount of data to/from cloud

--------------- S3 Features and oprational facts -------
1. S3 Performance --> S3 can scale upto very high request rates, best practice is to avoid account limits and optimize solution
    --> contact AWS support for temporary limit concerns, below is what default limits:
          300 request per sec --> PUT/LIST/DELETE
          800 request per sec --> GET
    --> revisit design or follow best practices incase you consistently hit high number of request to optimize solution 
         100 request per sec --> PUT/LIST/DELETE
         300 request per sec --> GET
  --- 2 types of workload options ---
    1. S3 Intensive Mixed request workload (PUT/LIST/DELETE/GET) -->
  solution --> Introducing randomness to avoid overwhelming S3 partitions
      --> S3 automatically creates and maintain an idex for S3 key-names in each AWS region
      --> The keys are stored across multiple partitions based on key-names
      --> adding random prefix can help spread load across
      --> prefix can be generated by using MD5 hash-code or any other approaches
    2. S3 GET Intensive Workload -->
      --> Same above, Introducing Randomness can help
      --> Use of AWS CloudFront -->
          --> As its Content Delivery Network, which delivers content with low latency and high transfer rate.
          --> Cache S3 objects, CloudFront will be closer to user location
    
2. S3 Permissions --> By default all buckets and objects are private only owner has access
  owner can give permissions using resource based policies (bucket level or S3 ACLs) or IAM policies
    1. IAM policies --> attached to users, groups, or/and roles
            --> can't attach to S3 bucket or objects
            --> can't grant access to anonymous user
            --> {JSON} and can contain details about S3 bucket/object using ARN (amazon resource name)
                  {arn:aws:s3:::bucket_name/object_name} // object name
                  {arn:aws:s3:::bucket_name} // bucket name
                  {arn:aws:s3:::bucket_name/*} // all objects inside bucket
                  {arn:aws:s3:::*} // all buckets
                  {arn:aws:s3:::bucket_name/object_name/${aws:user_name}} // using variable inside json
                  
  Resource-based policies - 2 ways -->
    2. Bucket Policies --> {JSON} attached to S3 bucket level (not to objects), means its automatically applied to all objects of bucket created by bucket owner. objects created by other than owner has to explicitly specify object level permission.
    --> can be used to grant accesss to IAM users or other AWS accounts
    --> policy specifies allow and deny permission, even can specify based on user as well. (like, which user can use PUT/DELETE, etc)
    3. S3 ACL Policies --> {XML} can be used to specify permissions at both bucket and object level
          --> ACLs can't deny permissions and can't grant conditional permissions
          --> most useful for Object level permissions, as this is the only way to attach a policy to objects

---- S3 Features --->
1. S3 Encryption --> 2 ways -- All handled in background by AWS SDK
   1. In-transit/client side --> SSL or client side encyption techniques
        1. Client Side Master Key (CSMK) --> client manage master key and unencrypted data never sent to AWS.
              upload/download steps with CSMK --> 
                  Random "Data key" generated at S3 client
                  Earlier generated Random "Data key" will be encrypted with client provided "CSMK"
                  S3 client encrypt the data using "data key"
                  data will be uploaded with details as object metadata (x-amz-meta-x-amz-key)
                  client download data file with its metadata
                  metadata will inform S3 client to use which "CSMK" to decrypt "data key"
                  "data key" is used to decrypt actual data objects
        2. AWS KMS-managed customer master key (CMK) --> 
              --> client gets a unique encryption key for each object({(1)application request}, {(2) response: key and cipherText Blob})
              --> a plaintext key used to encrypt objet data
              --> encypted data will be uploaded with a cipherText blob as object Metadata
              --> Download file and metadata
              --> KMS decrypts ciphertext blob for client, to get the plaintext key
              --> plaintext key is used to decrypt the object data file
   2. At rest / server side --> S3 encrypt data for user / server side encryption
      SSE-S3 --> Amazon S3 provides encryption key before saving data to disk 
            --> AES-256 (Advanced Encryption Standards)
            --> each object gets a unique key that is encrypted by regulatory-rotated master key
            --> every upload request will have request header as "x-amz-server-side-enryption"
      SSE-KMS --> similar as SSE-S3, but key managed by AWS-KMS service, provides better control, better auditing for access
      SSE-C --> customer provides and manages master key, S3 encrypts and decrypts using that key

2. S3 versioning --> a feature to keep old/new/deleted S3 objects
  by default, its disabled. But once enabled, you can only suspend it, can't disable it.
  Hence, suspending versioning mean, not creating any new version, but old versions will be maintained as-is
  versioning can only be set at bucket level and will be applicable to all objects
  Lifecycle policies can be applied to specific version of the objects
  every object is independent in itself and consist of unique version id
  Enable Versioning -->
     Existing object remain unchaged and have null as versionID
     Each addedd objects will automatically have unique versionID
     VersionID are generally generated by S3
     Every new version is billed as full new S3 object
     Each version object is independent from other object and complete in itself not a partial part of object
     Any object can be fetched using object key and VersionID
  Deleting Versioned objects --> 
    1. if delete is made to an object without specifying version id, then AWS will add a delete marker to all versions of object
    2. when we try to access object, S3 will response 404, but you can retrieve specific version of object by specifying version id
    3. each version of object can permanently be deleted only by specifying version id
  Restoring versioned objects --> means making old version of object to current, can be achieve in 2 ways
    Deleting current versions --> keep deleting untill you get needed object version
    copying old version --> each copied objects (even from same bucket) will become new version of object with unique version id

3. S3 storage classes and Glacier -->
  storage class --> represents classification of object in S3, this dictates storage cost, availability, durability, frequency of access
              --> each object must be assigned to a storage class.
      Durability --> can be described a the probability that you will eventually be able to get your object back from the storage system from one of the stores and archives.
      Availability --> is the probability that you will be able to get it back the moment that you ask for it.
  4 class types -->
  S3-Standard --> default and for all purpose, 11X9's durable, 99.99% available, most expensive 
  S3-RRS (Reduced Redundency Storage)--> designed for non-critical/reproducible objects. 99.99% available/durable, little less expensive
  S3-IA (Infrequent Access) {standard-IA} --> designed for IA but with immidiate availability when requested, 11X9's durable, 99.90% available, less expensive than standard/RRS
    OneZone-IA --> all S3-IA specification applied to one specific zone only
    S3-Glacier --> long time archival storage, should not be used for back-ups, cheapest, several hours of retrieval time, 11X9's durable
   3 types of retrieval --> expedited (1-5 min), standard (3-5 hr), bulk (5-12 hr)

Settings/changing storage class --> while uploading to S3, default storage class is Standard.
    you can perform manual setting or use life cycle policy to update/change storage class for object.
    ** to move to Glacier storage class, can be done only by lifecycle policy and it can take 1-2 days to take effect.

4. S3 lifecycle policy --> (by default disabled), set of rules that automates migration of objects to various storage classes
      --> good for company data retention policy, cost efficient, ease of management, 

5. S3 hosting static website --> you can build a code that can nevigate to different files (usually HTML, CSS, JavaScript) stored at S3.
      --> index.html (or default file) and error.html (error file for 4XX errors) must be specified.
       URL format --> bucket_name.s3-website(- or.)region.amazonaws.com
       URL example --> my-cool-site.s3-website-us-east-1.amazonaws.com
    --> AWS Route 53 can be used to map human redable domain names with this URL, also helpful in DNS failover situation
    
  Cross Origin Resource Sharing (CORS) --> settings at S3 bucket level, {XML} file
    a method that allow web application located at one domain to access and use resources at another domain
    or in AWS world, one S3 bucket can access resources of another S3 bucket
    Just need to Enabling CORS at S3 bucket level 
    
6. S3 Events --> used to setup automated notifications between S3 and other AWS services/resources based on some event occurance.
    common example:- RRSObjectLost (used for automating the recreation of lost RRS objects)
                     ObjectsCreated (in-case of any of these API calls -- PUT/POST/COPY/CompleteMultiPartUpload
    --> Event notification can be send to following AWS services (SNS/Lambda/SQS Queue)
