DynamoDB --> NoSQL Db
Benfits --> No management of DB, provisioning of Software and Hardware. 
      --> Automatically scales throughput capacity
      --> Synchronously replicates data across 3 facilities
      --> supports in-place atomic updates (means for any data update, you will either get new data or old data. you will never get partially updated data.) 
      --> supports automic increment / decrement operation on scalar values.
      --> low latency read and write access from 0 bytes to 400 KB.
      
Data Consistency Model -->
  1. Eventually consistent reads (default) --> maximizes read throughput. within a second data will be write and reflected to all copies.
  2. Strongly Consistent reads --> read only after successfull write across all.

what are the things reflects Cost of Dynamodb --> 
    1. build on SSD , 2. three replication copies , 3. Fault tolerant, highly scalable and available

Is DynamoDB only for High-scale application --> No, it offer seemless scaling for any size of applications. But it also depends on type of application for suitability.

What kind of Query functionality supported 
  --> "GET/PUT" oprations, with Primary Key as required field input.
  --> allows query based on LSI (Local secondary index) and GSI (Global Secondary Index)

Few DynamoDB APIs -->
    CreateTable – Creates a table and specifies the primary index used for data access.
    UpdateTable – Updates the provisioned throughput values for the given table.
    DeleteTable – Deletes a table.
    DescribeTable – Returns table size, status, and index information.
    ListTables – Returns a list of all tables associated with the current account and endpoint.
    PutItem – Creates a new item, or replaces an old item with a new item (including all the attributes). If an item already exists in the specified table with the same primary key, the new item completely replaces the existing item. You can also use conditional operators to replace an item only if its attribute values match certain conditions, or to insert a new item only if that item doesn’t already exist.
    BatchWriteItem – Inserts, replaces, and deletes multiple items across multiple tables in a single request, but not as a single transaction. Supports batches of up to 25 items to Put or Delete, with a maximum total request size of 16 MB.
    UpdateItem – Edits an existing item's attributes. You can also use conditional operators to perform an update only if the item’s attribute values match certain conditions.
    DeleteItem – Deletes a single item in a table by primary key. You can also use conditional operators to perform a delete an item only if the item’s attribute values match certain conditions.
    GetItem – The GetItem operation returns a set of Attributes for an item that matches the primary key. The GetItem operation provides an eventually consistent read by default. If eventually consistent reads are not acceptable for your application, use ConsistentRead.
    BatchGetItem – The BatchGetItem operation returns the attributes for multiple items from multiple tables using their primary keys. A single response has a size limit of 16 MB and returns a maximum of 100 items. Supports both strong and eventual consistency.
    Query –  Gets one or more items using the table primary key, or from a secondary index using the index key. You can narrow the scope of the query on a table by using comparison operators or expressions. You can also filter the query results using filters on non-key attributes. Supports both strong and eventual consistency. A single response has a size limit of 1 MB.
    Scan – Gets all items and attributes by performing a full scan across the table or a secondary index. You can limit the return set by specifying filters against one or more attributes.
    ConditionExpression --> to specify conditional aurguments
    KeyConditionExpression --> to filter out records based on key values
    ProjectionExpression --> to determine which attributes should be retrieved from the table. 
    
DynamoDB vs RDS -->
  DynamoDB fits --> 
       --> fully managed and grow as per application requirment.
       --> Highly scalable (no schema based), Highy available, etc.
       --> Customer simply store and request data as per application need.
       --> Running adhoc query on DynamoDB is inefficient.
      --> no limit on request (read/write per sec) capacity and storage capacity. Customer can manage as per need.
      
  RDS -->
      --> is useful for robust feature and functionality. means running adhoc queries, etc. 
      --> for query flexibility (OLAP)

DynamoDB vs S3 -->
    DynamoDB --> 
        --> stores structured data, indexed with primary key
        --> low latency read and write access from 0 bytes to 400 KB.
        --> S3 file pointers can also stored in DynamoDB to access fast 
    S3 -->
      --> suitable for unstriutured data 
      --> storing large objects upto 5TB.
      --> large objects and infrequent accessed data sets, needs to be stored in S3
 
 DynamoDB data model -->
    Table --> collection of data items. No limit on number of items in a table. Schema-less table. Must have Primary Key.
    Item --> composed of primary key or composite key, with flexible number of attributes. An Item size can't exceed 400 KB.

Scan operation --> works like an iterator, each call will return max of 1 MB result.
      --> read capacity unit = { number of bytes fetched by scan (rounded of, in multiple of 4KB) } / 4 KB
      --> Consistent reads consumes twice the read capacity as compared to evetually consistent reads.
      
      consistency model for Scan opration --> default eventual consistent
            --> using ConsistentRead = True, parameter will make it Strongly Consistent read.
            
DynamoDB supported Data types --> Number, String, Binary, and Boolean. collection data types: Number Set, String Set, Binary Set, heterogeneous List and heterogeneous Map. DynamoDB also supports NULL values.

 DynamoDB supported data structure -->  key-value and document data structures.
           Key-Value store -->  Key identifing specific data item and Value as actual data content
           document format -->  such as JSON, XML, and HTML.

JSON is not DynamoDB data type but it supports JSON data documents.
Querying JSON Data --> Need to create LSI or GSI on top on JSON Data file. to access it using queries.

Document SDK is a datatypes wrapper for JavaScript that allows easy interoperability between JS and DynamoDB datatypes. 

Scalability, availability, and durability sections -->
            --> no storage limit
            --> no throughtput limit, but If you wish to exceed throughput rates of 10,000 writes/second or 10,000 reads/second, then talk to AWS
            --> 100% sever failure and fault tolerant , AZ outage
Auto Scaling --> 
            --> ensure application availability and reduces costs from unused provisioned capacity.
            --> deally suited for request patterns that are uniform, predictable, with sustained high and low throughput usage that lasts for several minutes to hours.
            --> By default it is enabled, else you can confugure target utilization and minimum and maximum capacity. 
            --> Configuration settings are available at "Capacity" and "Indexes" tab
            --> Every chnage in auto scaling policy creates an CloudWatch alarm which activates auto scaling activity 
            --> Auto Scaling policy can only be set to a single table or a global secondary indexes within a single region.
            --> scaling up and down to maximum / minimum capacity is not supported. Instead temporary disable Auto scaling for those scenarios and allocate needed capacity.
            --> reserved capacity will be billed at discounted rate and any extra capacity used at regular price

Global Secondary Index (GSI) --> 
      --> contain a partition or partition-and-sort keys that can be different from the table's primary key.
      --> Amazon DynamoDB creates and maintains indexes for the primary key attributes
      --> User can create Secondary indexes --> 
           LSI --> same partition key as the table, but a different sort key
                  --> this increases query efficiency
                  --> overs items that are stored together (on the same partition) but different sort key
                  --> total size limit 10 GB per partition key value
                  --> LSI affected items are atomically updates (means either user will see new data or old data, not partially updated data)
                  --> allows retrieve attributes that are not part of the projection list.
                  --> All scalar data types (Number, String, Binary, and Boolean) can be used for the sort key element of the local secondary index key. Set, list, and map types cannot be indexed.
                 --> LSI must consiste of 3 entries (1) the table partition key value, (2) an attribute to serve as the index sort key, and (3) the table sort key value.
                 --> LSI total can consist upto 20 attributes including key and non-key attributes
                 --> supports both strong and eventually consistent read options.
                 --> LSI supports only QUERY operation.
                 --> LSI must be defined at time of table creation. Not possible to create LSI on existing table.
                 --> maximum of 5 LSI per table
                 --> 20 projected non-key attributes, in total across all local secondary indexes within the table
                 
                 
                 
           GSI --> contain a partition or partition-and-sort keys that can be different from the table's primary key.
                  --> increases query flexibility by enabling queries against any non-key attributes (non-unique attribute)
                  --> GSI does not need to have a sort key element.
                  --> useful for tracking relationships between attributes that have a lot of different values
                  --> maximum of 5 global secondary indexes per table
                  --> GSI not limited to any attibute of table, it span across table
                  --> GSIs do not enforce data co-location, and have no size restriction.
                  --> GSI is eventually consistent, means updates within few sec in all tables. (updates are asynchronous)
                  --> not allows retrieve attributes that are not part of GSI
                  --> GSIs manage (provision) throughput independently of the table they are based on.
                  --> One query can use only one GSI
                  --> GSI supports both Query and Scan operation. but scan operation will not fetch not projected GSI attributes.
                  ---> For GSI, ConsistentRead parameter can't be used as it support eventual consistency.
                  --> GSI can be chnaged at any time
                  --> Auto scling is recommended for GSI, Adding or deleting index also require additional throughput if setup manually.
                  --> DescribeTable API will show status of Index. "ACTIVE" means Index has been created.
                  --> Index creation process can't be canceled.
                  --> Composite attribute Index are not possible
                  --> GSI support all data types (including set types)
      
      CloudWatch metrics that are supported by a table. These include:
            Read Capacity (Provisioned Read Capacity, Consumed Read Capacity)
            Write Capacity (Provisioned Write Capacity, Consumed Write Capacity)
            Throttled read events
            Throttled write events
            
      projected attributes --> secondary index consists of attributes copied 
     
     GSI and provisioned capacity of table -->
            --> GSI item will consume write capacity units based on the size of the update. The capacity consumed by the GSI write is in addition to that needed for updating the item in the table.
            --> If GSI attributes are not impact by table operation then, GSI wouldnt consume any capacity.
        
        --> UpdateTable or PutScalingPolicy CLI can be used
        --> 



DynamoDB Streams --> time-ordered sequence of item-level data changes in a table in the last 24 hours
      --> have to be enabled on a per-table basis
      --> stream is managed automatically in DynamoDB
      --> DescribeStream API to check status of stream. "ENABLED" means active status
      
Benefits of DynamoDB Streams -->
     --> help developers to consume updates and receive the item-level data before and after items are changed.
     --> DynamoDB Streams read capacity is twice of the provisioned write capacity of your DynamoDB table. 
     --> DynamoDb stream will be available for next 24 hours even after deletion of DynamoDB table to consume updated data.
     --> there will be no gap in the data updates in DynamoDB stream
     --> DynamoDB stream contains information about both the previous value and the changed value of the item, with Primary Key and change type (INSERT, REMOVE, and MODIFY)
     
Limitation of Stream -->
      --> infomration defination (type) cant be changed in a stream after it has been created. you need to create new stream defination in that case.
      --> 
     
few additional terms -->
Elasticsearch --> is a popular open source search and analytics engine designed to simplify real-time search and big data analytics. 
Logstash --> is an open source data pipeline that works together with Elasticsearch 

Use Dynamodb as Storage backend for Titan --> plug-in that allows you to use DynamoDB as the underlying storage layer for Titan graph database. 

A graph database is a store of vertices and directed edges that connect those vertices. Both vertices and edges can have properties stored as key-value pairs.

Benefit of Using Dynamodb as Storage backend for Titan --> Dynamodb is storage managed service for titan. Hence, no hustle on cluster conf.

DynamoDB supports cloudWatch monitoring and these matrics are reported in either 1-minute or 5 min intervals

Tagging --> A tag is a label you assign to an AWS resource. Its a Key-Value type, each key should be unique.
      --> AWS uses tags as a mechanism to organize your resource costs on your cost allocation report. 
      --> DynamoDB Streams usage cannot be tagged at present.
      --> DynamoDB Reserved Capacity charges per table will show up under relevant tags.
      --> data usage charges are not tagged. This is because data usage is billed at an account level and not at table level.
      --> tag values can be null.
      --> You can add up to 50 tags to a single DynamoDB table. prefix “aws:” cannot be manually created

Time-To-Live (TTL)
      --> DynamoDB Time-to-Live (TTL) is a mechanism that lets you set a specific timestamp to delete expired items from your tables.
      --> no manual interaction needed for few data screnarios. That works on timely basis.
      Note: TTL requires a numeric DynamoDB table attribute populated with an epoch timestamp to specify the expiration criterion for the data. 
      --> this attribute can be projected as index attribute.
      --> Updating items with an older TTL values is allowed.
      --> if you try to read items that have expired but haven’t yet been deleted, the returned result will include the expired items.
      --> DynamoDB stream entry will be written at the point of deletion, not the TTL expiration time
      --> Expired items are not backed up before deletion.
      --> The scan and delete operations needed for TTL are carried out by the system and does not count toward your provisioned throughput or usage.
      
Delete operation vs TTL -->
     --> If you want to delete item immidiately apply delete operation.
     -->  TTL doesn't guarantee on the actual deletion timeframe
     
Limitation of TTL
      --> this is for item level deletion not table level
      --> do not support specifying a TTL attribute in a JSON document

DynamoDB Accelerator (DAX) -->
      --> in-memory cache for DynamoDB for fast in-memory performance (fully managed, highly available, fault-tolerant and scalable)
      --> application needs to make DAX client call, instead of direct DynamoDB call
      --> All read API calls will be cached by DAX, with strongly consistent requests being read directly from DynamoDB, while eventually consistent reads will be read from DAX if the item is available. 
   --> Within a DAX cluster, there are two different caches: 
  1) item cache --> The item cache manages GetItem, PutItem, and DeleteItem requests for individual key-value pairs. 
  2) query cache --> The query cache manages the result sets from Scan and Query requests.
   --> supported languages, Java, Node.js, Python, and .NET 
  Benefits -->
      --> repeat reads of cached data can be served immediately with extremely low latency
      --> DAX is DynamoDB compatible, means DAX engine is designed to support the DynamoDB APIs for reading and modifying data in DynamoDB.
      --> Data is aged or evicted from DAX by specifying a Time-to-Live (TTL) value or evicted based on the Least Recently Used (LRU) algorithm.
      --> supports both eventually consistent or strongly consistent:
      --> DAX supports using AWS CloudFormation
      --> high availability (multi-AZs)
      
    DAX TTL vs DynamoDB TTL -->
      DynamoDB TTL --> user specify expiration time of item
      DAX TTL --> user specify upto what duration item should be cached, once it gets cache.
      
  Limitation -->
      --> Operations for table management such as CreateTable/DescribeTable/UpdateTable/DeleteTable are not supported.
      --> no support for cross-region replication
      
DynamoDB Global Tables -->
      DynamoDB Global Tables is a new multi-master, cross-region replication capability
      --> perform reads and writes to DynamoDB in AWS regions around the world, 
      --> one replica table per region
      --> 
      Limitation --> Global Tables does not support cross-account access. Multi-region access within same account.

DynamoDB Backup and restore --> 
      two ways -->
           1. point-in-time recovery (PITR) --> continuous backups of your table up to the last 35 days
           2. on-demand backup --> custom, and long-term archival requirements for regulatory compliance. you can retain, until you delete those backups.
      --> backup process always writes to a new table.
    Benefits --> backups does not consume provisioned read or write capacity
    Limitation --> Currently, you must recover a table from backup to the same AWS Region within the same AWS account where you took the backup.
      
DynamoDB encryption at rest -->
      --> Encryption at rest automatically integrates with AWS Key Management Service (KMS) for managing the keys used for encrypting your tables.
      --> managed server side encryption feature using AWS KMS keys stored in your AWS account, using 256-bit AES encryption.
      --> DynamoDB On-Demand Backups can be encrypted
      --> DynamoDB uses a single service default key for encrypting all of your DynamoDB tables.
      --> Encryption at Rest works at a table level granularity.
      
      pricing --> No additional DynamoDB charges, but KMS charges will apply for using a service default key. 
      
      Limitation --> 
            --> cannot enable encryption at rest for DynamoDB Streams.
            --> Encryption at Rest works at a table level granularity.
            --> you cannot disable encryption at rest on an encrypted table.
            
VPC endpoints for Amazon DynamoDB -->
      VPC endpoints --> logical entities within a VPC that create a private connection between a VPC and DynamoDB,
                  without requiring access over the internet, through a network address translation (NAT) device, or a VPN connection.
          --> supports same AWS Region as the VPC.
     Benfits -->
            --> support AWS Identity and Access Management (IAM) policies
            --> data packets between DynamoDB and your VPC will remain in the Amazon network.
            --> each VPC endpoint supports one service.
            --> VPC endpoints for DynamoDB support all fine-grained access control access keys.
            
     Limitation --> cannot access Amazon DynamoDB Streams using VPC endpoints
              --> Amazon DynamoDB does not support resource-based policies pertaining to individual tables, items, and the like S3.
              
              
  Q. Can I have multiple VPC endpoints for DynamoDB in a single VPC?
   Answer:- Yes, you can have multiple VPC endpoints for Amazon DynamoDB in a single VPC. Individual VPC endpoints can have different VPC endpoint policies. For example, you could have a VPC endpoint that is read-only and one that is read/write. However, a single route table in a VPC can only be associated with a single VPC endpoint for DynamoDB, because that route table will route all traffic to DynamoDB through the specified VPC endpoint.
      

      
      
      
      
