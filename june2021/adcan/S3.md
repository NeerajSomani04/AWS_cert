# Simple Storage Service (S3)
- It provides a near infinitely scalable object storage platform - accessible from anywhere with a public internet connection.
- read all S3 basics from this link (https://github.com/acantril/aws-sa-associate-saac02/blob/master/04-AWS-Fundamentals/00_LearningAids/S3Basics.pdf)
- **S3 is PRIVATE by default**
- by default only the person who created the bucket originally has access to it, permission to all other identity needs to be provided seperately.


### S3 Security (Resource Policies & ACLs)
- S3 Security is controlled via a combination of Identity Policies, Bucket Policies (Resource Policies) and Legacy Bucket and Object ACLs
- Bucket Policy Examples : https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html
- S3 bucket policy is a type of resource policy.
- A resource policy is just like an identity policy, but as the name suggest they are attached to resources instead of identities.
- The difference between resource policy and identity policy is all about perspective.
- Resource policy provides resource perspective permissions.
- With identity policy you are controlling what that identity can access, with resource policy you are controlling who can access that resource.
- Resource policy allow or deny permission to resources in same account or different accounts
- Resource policies can also allow or deny anonymous principals. 
  - Here principles means, AWS services like lambda, Ec2, etc, but not limited to AWS. Any other external application can also get permission, example, gmail account, dropbox, etc.
  - resource policies can be used to open a bucket to the world by referencing all principles even those not authenticated by AWS.
- Resource policy has one major difference to identity policy, that is presence of explicit "principal" component in the policy statement.
- There can be only one bucket policy per bucket, but it can have multiple statements.

### Identity Polices
- you can only attach identity policies to identities in your own account
- Hence, identity policy can only control security inside your own account.


## ACL (Access Control List)
- Its a way to apply security to objects and buckets
- They are sub-resource of that object or of that bucket
- Legacy, means its old, AWS don't recommend using it and prefer to use bucket policies instead.
- The reason it got removed, because its very inflexible and can perform very simple permission configurations. for example, they can't have conditions like bucket polcies.
- In general, there are only 5 permissions can be applied using ACL. which itself is very limited.
- ACL can be applied only on single object or single bucket. You can't apply it to group of objects or group of buckets.

### Block Public Access Settings
- This can be applied to only anonymous principles, meaning the identity or services that are not authenticated by your AWS account.
- The first out of four option, is to block public access that could be caused by any new ACL implemented.
- second option, to block public access granted by any ACL either new or old
- third option, allows any existing public access granted by bucket policies or access point policies. 
- fourth option, blocks both any existing and new bucket policies, from granting any public access.

### Exam powerup 
- Identity : Controlling different resources (bucket)
- Identity : You have a preference for IAM
- Identity : Same account
- Bucket : Just controlling S3
- Bucket : Anonymous or cross account
- ACLs : Never - Unless you must

### S3 Static Website Hosting
- Accessing S3 is generally done via APIs
- Static Website Hosting is a feature of the product which lets you define a HTTP endpoint, set index and error documents and use S3 like a website.
- S3 Pricing : https://aws.amazon.com/s3/pricing/
- Website endpoint is created
- custom domain via R53 - Bucketname matters

# S3 pricing
- based on storage
- based on data transfer, transfer data into S3 is free, but transfer data out of S3 is charged
- based on request operations (get, post, put, delete)
- based on management and analytics (s3 data analytics features)

### S3 Object versioning & MFA Delete
- Object versioning is a feature which can be enabled on an S3 bucket - allowing the bucket to store multi versions of objects
- These objects can be referenced by their version ID to interact directly - or omit this to reference the latest version of an object
- Objects aren't deleted - object deletion markers are put in place to hide objects.
- By default versioning is disabled, and user can enable it as per need. But once enabled, it can't be disabled.
- Although, versioning can be suspended and can be re-enabled again.
- When versioning of object is disabled, ID of the object is "NULL". But when versioning is enabled, every action (update/delete) creates new ID (random unique value) for that object to maintain versioning.
- If you want to retrieve a specific version of the object then just pass ID information as well in you request. If you don't specify ID means, you want the latest version of the object.
- In case of delete operation, S3 just put a delete marker on that specific object. This will hide the object from accessing it. but in reality this object is not deleted from bucket. you can remove this delete marker from object to retrieve this object back be accessible.
- Its possible to delete an object permanently, by specifying its ID with delete action.
- **MFADelete** 
  - its enabled in versioning configuration
  - means, MFA is required to change bucket versioning state
  - MFA is required to delete versions

### S3 performance optimization
- how S3 Uploads (PutObject) works ?
  - by default, any object is uploaded as single blob of data stream to S3
  - the problem with this is, if a stream fails, whole upload fails, and need to restart the full upload
  - Data Transfer protocols like, BitTorrent developed to allow speedy distributed transfer of data.
  - also, the limit of single put upload is 5 GB, but that itself is highly un-reliable
- Single PUT Upload
  - most of above points
- Multipart Upload
  - this is solution to above issue
  - This can be confugured, so that a large data can be divided into multiple small parts
  - min data size is 100 MB, meaning multi part can only be used if your original (full) file size is equal or more than 100 MB.
  - an upload can be divided into maximum of 10,000 parts. example, range between 5MB to 5GB
  - last part can be smaller than 5 MB
  - parts can fail, and be restarted
  - Transfer rate = speeds of all parts (highly improved)
- S3 Transfer Acceleration
  - it uses the convenience network of Edge location, end-user will upload data to edge location, and then it will be transferred to destination using AWS own reliable and fast network.
  - An S3 bucket needs to be enabled for transfer acceleration, by default its disabled.
    - The enabling of transfer acceleration comes with some restrictions and rules
      - bucket name can not contain periods
      - it needs to be DNS compatible in its naming
  - AWS Accelerated Transfer Tool : http://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html

### Key Management Service (KMS)
- its a regional and public service (but of-course you need permissions to access it)
- It allows user to create, store and manage keys
- these keys are used to convert plaintext to ciphertext and vice versa
- KMS is capable of handling both Symmetric and Asymmetric keys
- it can also be used for all cryptographic operations like encrypt, decrypt, etc
- **Exam imp point** -- Keys never leave KMS, it can only be used securely - provides FIPS 140-2 (L2)(security standard)
- Its also capable of managing CMK (Customer Master Key)
  - Its a logical item that contains few other items - ID, date, policy, description, state
  - every CMK is backed by physical key material
  - This key material can be generated by KMS or can be imported in KMS to encrypt or decrypt data
  - **exam imp** CMKs can be used to encrypt ot decrypt data for up to 4KB of size.
  - How to overcome this limitation
  - Use Data Encryption Keys
- Data Encryption Keys (DEKs)
  - This is also managed in KMS
  - This key can be generated inside KMS using CMK
  - This DEK can be utilized on larger data set (more than 4 KB)
  - DEK is always linked with some CMK
  - **most imp** KMS does not store the DEK in any way, KMS only provides it to user or service who requested it and then discards it.
  - The reason KMS doesn't keep it because it actually doesn't perform the encryption or decryption using DEK
  - It will be done by user or service who requested DEK

### Summary of KMS key concepts 
- CMKs are isolated to a region & never leave
- AWS managed CMKs or Customer Managed CMKS
- CMKs are more configurable, example edit key policy time to time
- Other AWS services can use the same CMK to perform operations.
- CMKs support key rotation
- rotation, is a process, in which actual physical key material is changed time to time.
- **exam imp** with AWS managed keys, rotation is always enabled (it can't be disabled) and it happend in every 3 years.
- **exam imp** with customer managed keys, rotation can be optional, but if enabled, it happens once a year.
- CMK itself contains current backing key (as well as any previous backing keys) 
- Alias can also be created to a particular CMK
- Key policies is also a type of resource policy
- Every CMKs has one key policy
- In every key policy you need to explicitely specify which AWS account that key trust. This is usually mentioned under principle component of key policy doc.
- with the combination of key policies and IAM policies, we can easily fine tune permissions of specific user.
- example, someone can encrypt the data using CMK but can't decrypt it. etc

### example of CMK commands
```
aws configure list-profiles ## This command is used to see what aws profiles are set in your environment.

# Shared
echo "find all the doggos, distract them with the yumz" > battleplans.txt

aws kms encrypt \
    --key-id alias/adcan-soa-kms-video \
    --plaintext fileb://battleplans.txt \
    --output text \
    --query CiphertextBlob \
    --profile iamadmin-general | base64 \
    --decode > not_battleplans.enc 
    
aws kms decrypt \
    --ciphertext-blob fileb://not_battleplans.enc \
    --output text \
    --profile iamadmin-general \
    --query Plaintext | base64 --decode > decryptedplans.txt
    
```

### S3 object Encryption
- Buckets are not encrypted, the object inside the buckets are encrypted
- AES-256 is the algorithm to encrypt the data.
- Client-Side Encryption
- SSE-C (Server side encryption with customer provided keys)
  - customer manage keys
  - s3 service manages the actual encryption/decryption process
  - user is required to provide object (file) and encryption key to encrypt the data
  - s3 just use the hash of the key and attach it to encrypted data. once encryption completed by S3, it discard the key. S3 never stores the key.
- SSE-S3 (Server side encryption with Amazon S3-managed keys)
  - Here, AWS S3 handles both management of key and encryption/decryption process of the data.
  - user just provide the data.
  - Here, master key, which encrypt the encryption key is also handled by S3
- SSE-KMS (Server side encryption with customer master keys {CMKs} stored in AWS-KMS)
  - This is very similar to SSE-S3
  - just one difference, here, customer master key {CMK} is managed by KMS. This master key is used to encrypt the encryption key which was generated by S3 and which was used to encrypt the data. So, this adds another layer of security.
  - This CMK is used for all the data that user upload. Unlike, SSE-S3, where everytime new key was generated by S3 and manage by S3.
  - This gives additional functionality of management of master key, meaning, you can define the key rotation criteria. you get the control and manage the master key as per need.
  - Hence, cloudTrail can also be used here to audit the master key management.
  - The biggest advantage is role seperation, everytime a user wants to encrypt or decrypt the data, the user must have permission to CMK in your KMS to perform that task. Hence, you get more control on these kind of permissions.
    - example, lets say, user1 has full permission on S3 but no permission on KMS, then user1 can't perform this task.
    - but if user2, who has access to S3 and KMS as well, then that user2 can perform this task. 
    - This is a big advantage, when you can allow some people to upload data that would be encrypted. But allow only few people to decrypt the data.

### default S3 bucket encryption functionality
- as part of object upload to S3, if you specify a specific header in your request, "x-amz-server-side-encryption", then you indicate S3 to perform Server side encryption on your data,
- This can be a default setting on your bucket level if you want, and AES-256 encryption will be used when you don't specify any encryption technique to use. This is by default S3 use, if you enable encryption. You can completely disable encryption as well.

### Some imp links to understand encryption in more details
https://docs.aws.amazon.com/AmazonS3/latest/user-guide/default-bucket-encryption.html

https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html

https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html

https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html

https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html

### S3 Object Storage Classes
- S3 Standard 
  - billed per GB/per month fee for data stored
  - per GB charge for data transfer out of S3 (IN is free)
  - price per 1000 requests
  - No specific retrieval fee, no minimum duration, no minimum size
  - replication over 3 AZ's and content-MD5 checksums and Cyclic Redundancy checks (CRCs) are used to detect and fix any data corruption.
  - **exam imp** This option should be used to frequently accessed data which is important and non replaceable.
- S3 Standard-IA (Infreqent Access)
  - Everything is almost same other than that its much cheaper than S3 standard plan
  - Also, there is a retrieval fee per GB of data, if you retrieve data frequently then this option becomes costly.
  - Also, there is a minimum duration charge of 30 days - object can be stored for less but the minimum billing always applies to 30 days.
  - Also, it has a minimum capacity charge of 128 KB per object.
  - **exam imp** This should be used for long lived data which is important but where access is infrequent.
- S3 One Zone-IA
  - Everything is almost same other than that its much cheaper than above two.
  - **exam imp** This should be used for long lived data which is less important or non-critical & easily replacable but where access is infrequent, because it also has retrieval fee.
- S3 Glacier **exam imp - all below points** 
  - they are cold objects and are not immidiately available for use.
  - Objects can't be made publicly accesible and any access of data (beyond object metadata) requires a retieval process and fee.
  - 40 KB min size and 90 days min duration for billing. 
  - Data in Glacier is retrieved to S3 standard-IA temorarily. There is always first byte latency = minutes or hours.
    - Expedited - 1 to 5 minutes latency
    - Standard - 3 to 5 hours latency
    - bulk - 5 to 12 hours latency
    - Faster - More expensive
  - It should be used for Archival data where frequent or realtime access isn't needed. Minutes or hours of retrieval time.
- S3 Glacier Deep Archive
  - 40 KB min size and 180 days min duration for billing. 
  - Data in Glacier is retrieved to S3 standard-IA temorarily. There is always first byte latency = hours or days.
    - Standard - upto 12 hours latency
    - bulk - up to 48 hours latency
  - Used for archival data that rarely if ever needs to be accessed - hours or days for retrieval eg: legal or regulation data storage.
- Intelligent-Tiering
  - It monitors and automatically moves any objects not accessed for 30 days to low cost infrequent access tier and eventually to archive or deep archive tiers
  - If object needs frequent accessibility they it will be moved back to standard tier and there is no retrieval fee for accessing it, only a 30 day minimum duration cost applied.
  - Intelligent tiering has a monitoring and automation cost per 1000 objects. In this there is a management cost for this automatic tiering management of objects.
  - This tiering is only suitable for unknown access pattern. meaning, if you have data with changing access pattern over a period of time, or you are not aware of that pattern, uncertain usage. example, an image/video gets few access in initial days and suddenly it gets viral. So this is an unknown or uncertain pattern.

### S3 lifecycle configuration
- its a set of rules consist of actions
- on a bucket or groups of objects
- transition actions - this will move objects across different tiering,
  - transition can happen only in downward direction
- expiration actions - this will delete or remove objects
- **exam imp** 
  - if object size is small then need to be careful with this tier transitioning, because you end up same cost
  - Also, remember there is 30 days minimum retention period in each tier before moving to lower tier, that also adds up to cost
  - to overcome above 30 days limitation, you need to create two rules, one to move object from standard to IA and then to glacier tier. Then this can be done in less than 30 days.

### imp links related to storage pricing
https://aws.amazon.com/s3/pricing/

https://aws.amazon.com/s3/storage-classes/

### S3 Replication
- allow objects to be replicated between a SOURCE and DESTINATION buckets in the same or different AWS accounts
- Two types of Repilcation
  - Cross-Region Replication (CRR) is the process used when Source and Destination are in different AWS regions
  - Same-Region Replication (SRR) is used when the buckets are in the same region.
- https://docs.aws.amazon.com/AmazonS3/latest/dev/replication-what-is-isnot-replicated.html
- https://docs.aws.amazon.com/AmazonS3/latest/dev/replication.html
- https://aws.amazon.com/about-aws/whats-new/2019/11/amazon-s3-replication-time-control-for-predictable-replication-time-backed-by-sla/
- Replication configuration details
  - this applied to source bucket only
  - define what is destination bucket for replication
  - an IAM role defined and assumed to perform this task
  - replication is encrypted (SSL)
  - In case of SRR, IAM role defination is enough to trust the source and destination perform this operation
  - But in-case of CRR, user should also define bucket policy to define access to different account and IAM role to perform the task
- What options available when replication is configured
  - All objects or all tags or all prefixes or subset
  - Which storage class by default should be use
  - we can define ownership of objects in bucket, default is the source account can access
  - Replication Time control (RTC), this adds a guaranteed 15 min replication SLA onto the process. Without this its a best effort process.
  - RTC also adds monitoring capability, means user can see which objects are queued for replication
- **exam imp**
  - replication is not retroactive, meaning replication only start from the time it gets configured/enabled. If there are objects already available in the bucket before replication enabled, those objects wouldn't get replicated.
  - Also, versioning needs to be ON if you want to enable the replication, and it should be ON for both Source and destination bucket.
  - Its a one-way replication process, only source bucket to destination bucket.
  - Replication can handle any type of objects, unencrypted or encrypted (SSE-S3 & SSE-KMS), although in-case of SSE-KMS extra configuration needed.
  - **But replication of objects that are using SSE-C is not possible, thats not allowed **
  - source bucket owner needs permissions to objects which needs to be replicated
  - NO system events gets replicated, like lifecycle management. Only user events gets replicated, meaning only user uploaded files get replicated. Not the files that moved from one tier to another tier by life-cycle management of S3.
  - It can't replicate any objects that are using Glacier or Glacier Deep Archive storage classes, because conceptually that is different storage product all together within S3.
  - Deletes actions are not replicated by default, but with additional configuration feature, you can even replicate delete marker objects as well.

### Why use replication
- SRR - could be use for Log Aggregation. Example, if you store logs from different systems into different S3 buckets, you can aggregate them in one bucket using this feature.
- SRR - can be used to perform some kind of Synchronization between prod and test accounts. May be replicate data frok prod to test periodically for analysis.
- SRR - could be used for strict sovereignty requirements, meaning some kind of regulations that can't allow you move your data out of a specific region to perform some analysis or some other operations.
- CRR - could be used for global resilience improvements, collect data from all regions and plan better for failure. Or analysis could be done at large scale.
- CRR - This can help in latency reduction, for global available application. like, instagram, etc

### S3 PreSigned URLs
- Presigned URL's are a feature of S3 which allows the system to generate a URL with access permissions encoded into it, for a specific bucket and object, valid for a certain time period.
- This is only useful, when a person needs short-term access to bucket and its objects.
- This can be used for GET (download) and PUT (upload) operations
- Mostly this provides permissions to a user to a private bucket, as and when needed.
- **exam imp**
  - you can create a presigned URL for an object that you have no access too, its fairly unusual. Because, the only requirement to generate a presigned URL is to specify the object name and expiry date and time. But this presigned URL will also don't have access to that object. This also has some utility in real life scenario.
  - Even you can generate a presigned URL for an object that doesn't even exist in S3 bucket. but of-course you will get an error while using that URL that "no such object/key exist". Which is obvious.
  - When using the URL, the permissions match the identity which generated it. 
  - So, in case if you received access denied error, it could mean that the generated identity never had access to that object or that identity doesn't have access right now at the moment you are using that presigned URL. Because, whenever a user uses the presigned URL, they permission of URL gets checked agaist the current permission of the identity who generated it at the moment of usage.
  - Don't generate with an IAM role, URL stops working when temporary role credentials expire. 
  - IAM User is a better or preferred way of generating the presigned URL.
  - example CLI command for generating presigned URL
    - aws s3 presign s3://animals4lifemedia1337/all5.jpg --expires-in 180


### S3 Select and Glacier Select
- S3 and Glacier Select allow you to use a SQL-Like statement to retrieve partial objects from S3 and Glacier.
- Here partial means, you want to fetch data based on some criteria or condition, similar to DB SQL
- **exam imp** -- retrieving a 5TB size object from S3 takes time and uses 5TB of data transfer fee.
- hence, it would be helpful if we can filter the record as per our need and reduce the data size which eventually more cost and time efficient.

### Cross-origin Resource Sharing (CORS)
- defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. 
- With CORS support, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources.
- Nice article to understand it better : https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS
- Origin - its the first URL that you visit on browser or try to access the application or resource
- same-origin requests are always allowed, defined on intial request
- CORS is even used with various AWS services interactions, because every service URL is considered as different origin. Example, S3 and API Gateway, requires some kind of CORS setting even though they are in same account or region.
- CORS configuration is processed in order, the first rule which matches is used.
  - example statement, looks like, { "AllowedOrigins":["\*"] }
- Two type of requests usually requires this configuration
  - Simple requests -- means, you can go ahead and directly using a cross-origin request.
  - Preflight & Preflighted requests -- in this your browser first sends and http(s) request to the other origin, and determine is it safe to make the request to other origin or not.
- **exam imp** Various components that are part of CORS configuration and responses
  - Access-Control-Allow-Origin --> this basically specifies which origin are allowed to make requests. "\*" is used for wild card.
  - Access-Control-Max-Age --> this specifies how long result of the request can be cached.
  - Access-Control-Allow-Methods --> list of methods that are allowed for CORS, GET, PUT, DELETE or "\*" can be used.
  - Access-Control-Allow-Headers --> used to indicate which HTTP headers can be used within the request.


### S3 Access Points
- a feature of S3, simplifies managing data access at scale for applications using shared data sets on S3.
- Access points are unique hostnames that customers create to enforce distinct permissions and network controls for any request made through the access point.
- link: https://docs.aws.amazon.com/AmazonS3/latest/dev/creating-access-points.html#access-points-policies
- Rather than 1 bucket with 1 bucket policy
- create many access points
- each with different policies (access point policies same as bucket policies), restrict identities to certain prefix, tags or actions based on need.
- each with different network access controls
- each access point has its own endpoint address
- **exam imp** created via console or via CLI
  - aws s3control create-access-point --name secretcats --account-id 123456789012 --bucket catpics
- Access point with VPC origin, access points can be configured for access via a VPC - requires a VPC endpoint. Access via this route can be enforced by endpoint policies.
- Any permission defines on Access point, also needs to be defined on bucket policies.

### S3 Inventory
- provides a flat file list of your objects and metadata, which is a scheduled alternative to the Amazon S3 synchronous List API operation. 
- Amazon S3 inventory provides comma-separated values (CSV) or Apache optimized row columnar (ORC) or Apache Parquet (Parquet) output files that list your objects and their corresponding metadata on a daily or weekly basis for an S3 bucket or for objects that share a prefix
- Helps you manage (at high level) your storage
- Inventory of objects & various optional fields
- example fields that can come in report:-- Encyption, size, last modified, storage class, version ID, replication status, object lock, etc.....
- This can be generated daily or weekly (can't be forced to generate monthly or even ad-hoc). It can only be generated with configured and scheduled manner. Can't be generated manually.
- Also, once configured it can take upto 48 hours to receive your first report
- multiple inventories can be setup, and they go to a target bucket
- these target bucket can be in same AWS account or in different AWS account and needs proper permission settings.. bucket policy and all
- Mostly this is useful for Auditing, compliance, cost management or any specific regulations requirement.

### S3 Events
- The Amazon S3 notification feature enables you to receive notifications when certain events happen in your bucket. 
- To enable notifications, you must first add a notification configuration that identifies the events you want Amazon S3 to publish and the destinations where you want Amazon S3 to send the notifications.
- You store this configuration in the notification subresource that is associated with a bucket.
- Can be delivered to SNS, SQS and Lambda functions
- **exam imp** - all below points
- Object created (PUT, Post, Copy, CompleteMultiPartUpload)
- Object Delete (\*, Delete, DeleteMarkerCreated)
- Object Restore (Post (Initiated), Completed)
- Replication (OperationMissedThreshold, OperationReplicatedAfterThreshold, OperationNotTracked, OperationFailedReplication)
- **Exam imp** AWS EventBridge is an alternative service and support more types of events and more features.

### S3 Access Logs
- Server access logging provides detailed records for the requests that are made to a bucket.
- Server access logs are useful for many applications.
- For example, access log information can be useful in security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill.
- Access logging can be enabled via the console UI or via PUT bucket logging.
- Logging is managed by a system known as S3 Log Delivery group, which read the logging configuration set on S3 bucket.
- S3 log delivery group should also need access to Target bucket, and this can be achieved by Bucket policy.
- This is helpful for auditing, access pattern of your customers, to understand the charges of S3 bill on these objects.
- life cycle management of these log files needs to be done manually, its not available currently as part of the product feature.

### S3 Object Lock
- You can use S3 Object Lock to store objects using a write-once-read-many (WORM) model.
- It can help you prevent objects from being deleted or overwritten for a fixed amount of time or indefinitely.
- You can use S3 Object Lock to meet regulatory requirements that require WORM storage, or add an extra layer of protection against object changes and deletion.
- This can only be enabled on 'new' buckets, if you need to enable it on existing bucket or objects then a support ticket is required.
- When you enable this feature, versioning needs to be enabled as well
- Once you enable it, you can't disable it and can't suspend versioning on the bucket
- WORM -- means, No delete and No overwrite
- Objects has 2 categories to manage Object retention (Retention period & Legal Hold)
  - you can have both enables, one or the other or none enabled
  - These can be set on Objects or bucket default
  - Retention period -
    - Specify days or/and years - as a retention period
    - **exam imp** There are 2 modes
      - Compliance mode 
        - Can't be adjusted, deleted or overwritten for the duration of retention period
        - It also mean that retention period can't be reduced and retention mode can't be adjusted as well for the duration of retention period.
        - so no changes at all to the object version or retention period settings, and this even include **the account root user** itself.
        - any changes can be done only after retention period expires.
        - its mostly used for audit & compliance reasons
      - Governance mode
        - its little less strict than compliance mode
        - everything is same as above but special permissions can be granted allowing lock settings to be adjusted
        - the API for this is --> s3:BypassGovernanceRetention
        - also, each request that comes for adjustment, needs to provide a header in the request
        - and that head is --> x-amz-bypass-governance-retention:true (console default)
        - Hence, its mostly used for process reasons or governance reasons
  - Legal Hold - 
    - In this you don't set retention period at all
    - instead, you set an object version - ON or OFF
    - If Legal hold is enabled on an object version, you can't delete or change object untill legal hold is Off.
    - API used to add or remove legal hold --> s3:PutObjectLegalHold
    - This is mostly useful in-case of prevention of accidental deletion of critical object versions.
- a bucket can have default object lock settings.





