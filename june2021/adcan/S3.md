# Simple Storage Service (S3)
- It provides a near infinitely scalable object storage platform - accessible from anywhere with a public internet connection.
- read all S3 basics from this link (https://github.com/acantril/aws-sa-associate-saac02/blob/master/04-AWS-Fundamentals/00_LearningAids/S3Basics.pdf)
- **S3 is PRIVATE by default**
- by default only the person who created the bucket originally has access to it, permission to all other identity needs to be provided seperately.


### S3 Security (Resource Policies & ACLs)
- S3 Security is controlled via a combination of Identity Policies, Bucket Policies (Resource Policies) and Legacy Bucket and Object ACLs
- Bucket Policy Examples : https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html
- S3 bucket policy is a type of resource policy.
- A resource policy is just like an identity policy, but as the name suggest they are attached to resources instead of identities.
- The difference between resource policy and identity policy is all about perspective.
- Resource policy provides resource perspective permissions.
- With identity policy you are controlling what that identity can access, with resource policy you are controlling who can access that resource.
- Resource policy allow or deny permission to resources in same account or different accounts
- Resource policies can also allow or deny anonymous principals. 
  - Here principles means, AWS services like lambda, Ec2, etc, but not limited to AWS. Any other external application can also get permission, example, gmail account, dropbox, etc.
  - resource policies can be used to open a bucket to the world by referencing all principles even those not authenticated by AWS.
- Resource policy has one major difference to identity policy, that is presence of explicit "principal" component in the policy statement.
- There can be only one bucket policy per bucket, but it can have multiple statements.

### Identity Polices
- you can only attach identity policies to identities in your own account
- Hence, identity policy can only control security inside your own account.


## ACL (Access Control List)
- Its a way to apply security to objects and buckets
- They are sub-resource of that object or of that bucket
- Legacy, means its old, AWS don't recommend using it and prefer to use bucket policies instead.
- The reason it got removed, because its very inflexible and can perform very simple permission configurations. for example, they can't have conditions like bucket polcies.
- In general, there are only 5 permissions can be applied using ACL. which itself is very limited.
- ACL can be applied only on single object or single bucket. You can't apply it to group of objects or group of buckets.

### Block Public Access Settings
- This can be applied to only anonymous principles, meaning the identity or services that are not authenticated by your AWS account.
- The first out of four option, is to block public access that could be caused by any new ACL implemented.
- second option, to block public access granted by any ACL either new or old
- third option, allows any existing public access granted by bucket policies or access point policies. 
- fourth option, blocks both any existing and new bucket policies, from granting any public access.

### Exam powerup 
- Identity : Controlling different resources (bucket)
- Identity : You have a preference for IAM
- Identity : Same account
- Bucket : Just controlling S3
- Bucket : Anonymous or cross account
- ACLs : Never - Unless you must

### S3 Static Website Hosting
- Accessing S3 is generally done via APIs
- Static Website Hosting is a feature of the product which lets you define a HTTP endpoint, set index and error documents and use S3 like a website.
- S3 Pricing : https://aws.amazon.com/s3/pricing/
- Website endpoint is created
- custom domain via R53 - Bucketname matters

# S3 pricing
- based on storage
- based on data transfer, transfer data into S3 is free, but transfer data out of S3 is charged
- based on request operations (get, post, put, delete)
- based on management and analytics (s3 data analytics features)

### S3 Object versioning & MFA Delete
- Object versioning is a feature which can be enabled on an S3 bucket - allowing the bucket to store multi versions of objects
- These objects can be referenced by their version ID to interact directly - or omit this to reference the latest version of an object
- Objects aren't deleted - object deletion markers are put in place to hide objects.
- By default versioning is disabled, and user can enable it as per need. But once enabled, it can't be disabled.
- Although, versioning can be suspended and can be re-enabled again.
- When versioning of object is disabled, ID of the object is "NULL". But when versioning is enabled, every action (update/delete) creates new ID (random unique value) for that object to maintain versioning.
- If you want to retrieve a specific version of the object then just pass ID information as well in you request. If you don't specify ID means, you want the latest version of the object.
- In case of delete operation, S3 just put a delete marker on that specific object. This will hide the object from accessing it. but in reality this object is not deleted from bucket. you can remove this delete marker from object to retrieve this object back be accessible.
- Its possible to delete an object permanently, by specifying its ID with delete action.
- **MFADelete** 
  - its enabled in versioning configuration
  - means, MFA is required to change bucket versioning state
  - MFA is required to delete versions

### S3 performance optimization
- how S3 Uploads (PutObject) works ?
  - by default, any object is uploaded as single blob of data stream to S3
  - the problem with this is, if a stream fails, whole upload fails, and need to restart the full upload
  - Data Transfer protocols like, BitTorrent developed to allow speedy distributed transfer of data.
  - also, the limit of single put upload is 5 GB, but that itself is highly un-reliable
- Single PUT Upload
  - most of above points
- Multipart Upload
  - this is solution to above issue
  - This can be confugured, so that a large data can be divided into multiple small parts
  - min data size is 100 MB, meaning multi part can only be used if your original (full) file size is equal or more than 100 MB.
  - an upload can be divided into maximum of 10,000 parts. example, range between 5MB to 5GB
  - last part can be smaller than 5 MB
  - parts can fail, and be restarted
  - Transfer rate = speeds of all parts (highly improved)
- S3 Transfer Acceleration
  - it uses the convenience network of Edge location, end-user will upload data to edge location, and then it will be transferred to destination using AWS own reliable and fast network.
  - An S3 bucket needs to be enabled for transfer acceleration, by default its disabled.
    - The enabling of transfer acceleration comes with some restrictions and rules
      - bucket name can not contain periods
      - it needs to be DNS compatible in its naming
  - AWS Accelerated Transfer Tool : http://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html

### Key Management Service (KMS)
- its a regional and public service (but of-course you need permissions to access it)
- It allows user to create, store and manage keys
- these keys are used to convert plaintext to ciphertext and vice versa
- KMS is capable of handling both Symmetric and Asymmetric keys
- it can also be used for all cryptographic operations like encrypt, decrypt, etc
- **Exam imp point** -- Keys never leave KMS, it can only be used securely - provides FIPS 140-2 (L2)(security standard)
- Its also capable of managing CMK (Customer Master Key)
  - Its a logical item that contains few other items - ID, date, policy, description, state
  - every CMK is backed by physical key material
  - This key material can be generated by KMS or can be imported in KMS to encrypt or decrypt data
  - **exam imp** CMKs can be used to encrypt ot decrypt data for up to 4KB of size.
  - How to overcome this limitation
  - Use Data Encryption Keys
- Data Encryption Keys (DEKs)
  - This is also managed in KMS
  - This key can be generated inside KMS using CMK
  - This DEK can be utilized on larger data set (more than 4 KB)
  - DEK is always linked with some CMK
  - **most imp** KMS does not store the DEK in any way, KMS only provides it to user or service who requested it and then discards it.
  - The reason KMS doesn't keep it because it actually doesn't perform the encryption or decryption using DEK
  - It will be done by user or service who requested DEK

### Summary of KMS key concepts 
- CMKs are isolated to a region & never leave
- AWS managed CMKs or Customer Managed CMKS
- CMKs are more configurable, example edit key policy time to time
- Other AWS services can use the same CMK to perform operations.
- CMKs support key rotation
- rotation, is a process, in which actual physical key material is changed time to time.
- **exam imp** with AWS managed keys, rotation is always enabled (it can't be disabled) and it happend in every 3 years.
- **exam imp** with customer managed keys, rotation can be optional, but if enabled, it happens once a year.
- CMK itself contains current backing key (as well as any previous backing keys) 
- Alias can also be created to a particular CMK
- Key policies is also a type of resource policy
- Every CMKs has one key policy
- In every key policy you need to explicitely specify which AWS account that key trust. This is usually mentioned under principle component of key policy doc.
- with the combination of key policies and IAM policies, we can easily fine tune permissions of specific user.
- example, someone can encrypt the data using CMK but can't decrypt it. etc

### example of CMK commands
```
aws configure list-profiles ## This command is used to see what aws profiles are set in your environment.

# Shared
echo "find all the doggos, distract them with the yumz" > battleplans.txt

aws kms encrypt \
    --key-id alias/adcan-soa-kms-video \
    --plaintext fileb://battleplans.txt \
    --output text \
    --query CiphertextBlob \
    --profile iamadmin-general | base64 \
    --decode > not_battleplans.enc 
    
aws kms decrypt \
    --ciphertext-blob fileb://not_battleplans.enc \
    --output text \
    --profile iamadmin-general \
    --query Plaintext | base64 --decode > decryptedplans.txt
    
```

### S3 object Encryption
- Buckets are not encrypted, the object inside the buckets are encrypted
- AES-256 is the algorithm to encrypt the data.
- Client-Side Encryption
- SSE-C (Server side encryption with customer provided keys)
  - customer manage keys
  - s3 service manages the actual encryption/decryption process
  - user is required to provide object (file) and encryption key to encrypt the data
  - s3 just use the hash of the key and attach it to encrypted data. once encryption completed by S3, it discard the key. S3 never stores the key.
- SSE-S3 (Server side encryption with Amazon S3-managed keys)
  - Here, AWS S3 handles both management of key and encryption/decryption process of the data.
  - user just provide the data.
  - Here, master key, which encrypt the encryption key is also handled by S3
- SSE-KMS (Server side encryption with customer master keys {CMKs} stored in AWS-KMS)
  - This is very similar to SSE-S3
  - just one difference, here, customer master key {CMK} is managed by KMS. This master key is used to encrypt the encryption key which was generated by S3 and which was used to encrypt the data. So, this adds another layer of security.
  - This CMK is used for all the data that user upload. Unlike, SSE-S3, where everytime new key was generated by S3 and manage by S3.
  - This gives additional functionality of management of master key, meaning, you can define the key rotation criteria. you get the control and manage the master key as per need.
  - Hence, cloudTrail can also be used here to audit the master key management.
  - The biggest advantage is role seperation, everytime a user wants to encrypt or decrypt the data, the user must have permission to CMK in your KMS to perform that task. Hence, you get more control on these kind of permissions.
    - example, lets say, user1 has full permission on S3 but no permission on KMS, then user1 can't perform this task.
    - but if user2, who has access to S3 and KMS as well, then that user2 can perform this task. 
    - This is a big advantage, when you can allow some people to upload data that would be encrypted. But allow only few people to decrypt the data.

### default S3 bucket encryption functionality
- as part of object upload to S3, if you specify a specific header in your request, "x-amz-server-side-encryption", then you indicate S3 to perform Server side encryption on your data,
- This can be a default setting on your bucket level if you want, and AES-256 encryption will be used when you don't specify any encryption technique to use. This is by default S3 use, if you enable encryption. You can completely disable encryption as well.

### Some imp links to understand encryption in more details
https://docs.aws.amazon.com/AmazonS3/latest/user-guide/default-bucket-encryption.html

https://docs.aws.amazon.com/kms/latest/developerguide/services-s3.html

https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html

https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html

https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerSideEncryptionCustomerKeys.html

### S3 Object Storage Classes
- S3 Standard 
  - billed per GB/per month fee for data stored
  - per GB charge for data transfer out of S3 (IN is free)
  - price per 1000 requests
  - No specific retrieval fee, no minimum duration, no minimum size
  - replication over 3 AZ's and content-MD5 checksums and Cyclic Redundancy checks (CRCs) are used to detect and fix any data corruption.
  - **exam imp** This option should be used to frequently accessed data which is important and non replaceable.
- S3 Standard-IA (Infreqent Access)
  - Everything is almost same other than that its much cheaper than S3 standard plan
  - Also, there is a retrieval fee per GB of data, if you retrieve data frequently then this option becomes costly.
  - Also, there is a minimum duration charge of 30 days - object can be stored for less but the minimum billing always applies to 30 days.
  - Also, it has a minimum capacity charge of 128 KB per object.
  - **exam imp** This should be used for long lived data which is important but where access is infrequent.
- S3 One Zone-IA
  - Everything is almost same other than that its much cheaper than above two.
  - **exam imp** This should be used for long lived data which is less important or non-critical & easily replacable but where access is infrequent, because it also has retrieval fee.
- S3 Glacier **exam imp - all below points** 
  - they are cold objects and are not immidiately available for use.
  - Objects can't be made publicly accesible and any access of data (beyond object metadata) requires a retieval process and fee.
  - 40 KB min size and 90 days min duration for billing. 
  - Data in Glacier is retrieved to S3 standard-IA temorarily. There is always first byte latency = minutes or hours.
    - Expedited - 1 to 5 minutes latency
    - Standard - 3 to 5 hours latency
    - bulk - 5 to 12 hours latency
    - Faster - More expensive
  - It should be used for Archival data where frequent or realtime access isn't needed. Minutes or hours of retrieval time.
- S3 Glacier Deep Archive
  - 40 KB min size and 180 days min duration for billing. 
  - Data in Glacier is retrieved to S3 standard-IA temorarily. There is always first byte latency = hours or days.
    - Standard - upto 12 hours latency
    - bulk - up to 48 hours latency
  - Used for archival data that rarely if ever needs to be accessed - hours or days for retrieval eg: legal or regulation data storage.
- Intelligent-Tiering
  -  

### imp links related to storage pricing
https://aws.amazon.com/s3/pricing/

https://aws.amazon.com/s3/storage-classes/


